{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load tests/test_tweetokenize.py\n",
    "#!/usr/bin/env python\n",
    "#\n",
    "# tweetokenize: Regular expression based tokenizer for Twitter\n",
    "# Copyright: (c) 2013, Jared Suttles. All rights reserved.\n",
    "# License: BSD, see LICENSE for details.\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "import unittest\n",
    "from tweetokenize import Tokenizer\n",
    "\n",
    "\n",
    "class TokenizeTests(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.tokenizer = Tokenizer(lowercase=True)\n",
    "    \n",
    "    def test_general_1(self):\n",
    "        self.tokenizer.normalize = 2\n",
    "        msg = ('omg wow &#x3c; &#x26; &#x3e; &#62;.&#60; &gt;.&lt; :):)'\n",
    "        'i CANT believe thatttt haha lol!!1')\n",
    "        tks = ['omg', 'wow', '<', '&', '>', '>.<', '>.<', ':)', ':)',\n",
    "        'i', 'CANT', 'believe', 'thatt', 'haha', 'lol', '!', '!', '1']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_general_2(self):\n",
    "        msg = \"i'm wanting to jump up and down but wouldn't if i couldn't..\"\n",
    "        tks = [\"i'm\", 'wanting', 'to', 'jump', 'up', 'and', 'down',\n",
    "        'but', \"wouldn't\", 'if', 'i', \"couldn't\", '...']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_urls_1(self):\n",
    "        msg = (\"hey bro chec'k out http://shitstorm.com its fucking sick\")\n",
    "        tks = ['hey', 'bro', \"chec'k\", 'out', 'URL', 'its', 'fucking', 'sick']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_urls_2(self):\n",
    "        msg = ('also see this crazy stuff https://shitstorm.com')\n",
    "        tks = ['also', 'see', 'this', 'crazy', 'stuff', 'URL']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_urls_3(self):\n",
    "        msg = 'hiiiii rayj.com/ihititfirst and other google.com http://hobo.net'\n",
    "        tks = ['hiii', 'URL', 'and', 'other', 'URL', 'URL']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_usernames_1(self):\n",
    "        msg = ('@justinbeiber yo man!! ! i love you in a totally '\n",
    "        'straight way <3:p:D')\n",
    "        tks = ['USERNAME', 'yo', 'man', '!', '!', '!',\n",
    "        'i', 'love', 'you', 'in', 'a', 'totally', 'straight', 'way',\n",
    "        '<3', ':p', ':D']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_usernames_2(self):\n",
    "        msg = '@heyheymango: what did you SAYYY??? or did you just..  NotHING?'\n",
    "        tks = ['USERNAME', ':', 'what', 'did', 'you', 'SAYYY', '?',\n",
    "        '?', '?', 'or', 'did', 'you', 'just', '...', 'nothing', '?']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_numbers_1(self):\n",
    "        self.tokenizer.numbers = None\n",
    "        msg = ('i have this much money -2.42 in my bank acct.,friend! but you '\n",
    "        'have mucho +88e44 and its about 1000% more than $400.')\n",
    "        tks = ['i', 'have', 'this', 'much', 'money', '-2.42', 'in',\n",
    "        'my', 'bank', 'acct', '.', ',', 'friend', '!', 'but', 'you',\n",
    "        'have', 'mucho', '+88e44', 'and', 'its', 'about', '1000%',\n",
    "        'more', 'than', '$400', '.']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_numbers_2(self):\n",
    "        msg = ('i have this much money -2.42 in my bank acct.,friend! but you '\n",
    "        'have mucho +88e44 and its about 1000% more than $400.')\n",
    "        tks = ['i', 'have', 'this', 'much', 'money', 'NUMBER', 'in',\n",
    "        'my', 'bank', 'acct', '.', ',', 'friend', '!', 'but', 'you',\n",
    "        'have', 'mucho', 'NUMBER', 'and', 'its', 'about', 'NUMBER',\n",
    "        'more', 'than', 'NUMBER', '.']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_numbers_3(self):\n",
    "        self.tokenizer.lowercase = False # keep cases the same everywhere\n",
    "        msg = ('I JUST want To Test FRACTIONZZZ 22432.41414/ 55894385e-341 also'\n",
    "        ' lowercase etc.etc.etc. hope that last part doesn\\'t parse as a url '\n",
    "        'i would be kinda sad PANda!zsss..... .. . .... 4/5 5.1/4.0e0 3.14 -2')\n",
    "        tks = ['I', 'JUST', 'want', 'To', 'Test', 'FRACTIONZZZ',\n",
    "        'NUMBER', 'also', 'lowercase', 'etc', '.', 'etc', '.', 'etc',\n",
    "        '.', 'hope', 'that', 'last', 'part', \"doesn't\", 'parse', 'as',\n",
    "        'a', 'url', 'i', 'would', 'be', 'kinda', 'sad', 'PANda', '!',\n",
    "        'zsss', '...', '...', '.', '...', 'NUMBER', 'NUMBER', 'NUMBER',\n",
    "        'NUMBER']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_time_1(self):\n",
    "        msg = 'is the time now 12:14pm? or is it like 2:42AM??'\n",
    "        tks = ['is', 'the', 'time', 'now', 'TIME', '?', 'or', 'is',\n",
    "        'it', 'like', 'TIME', '?', '?']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_time_2(self):\n",
    "        msg = 'new time is 2:42:09 PM!!'\n",
    "        tks = ['new', 'time', 'is', 'TIME', '!', '!']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_phonenumber_1(self):\n",
    "        msg = ('my number is 18002432242 and 241.413.5584 also 1-242-156-6724'\n",
    "        ' and (958)555-4875 or (999) 415 5542 is 422-5555 a 131-121-1441')\n",
    "        tks = ['my', 'number', 'is', 'PHONENUMBER', 'and', 'PHONENUMBER',\n",
    "        'also', 'PHONENUMBER', 'and', 'PHONENUMBER', 'or', 'PHONENUMBER',\n",
    "        'is', 'PHONENUMBER', 'a', 'PHONENUMBER']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_phonenumber_2(self):\n",
    "        msg = 'numbers with extension: (201)-340-4915 x112 or 1 800.341.1311x99'\n",
    "        tks = ['numbers', 'with', 'extension', ':', 'PHONENUMBER', 'or',\n",
    "        'PHONENUMBER']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_quotes_1(self):\n",
    "        self.tokenizer.ignorequotes = True\n",
    "        msg = 'this is just a tweet with \"someone said something funny\" lol'\n",
    "        tks = ['this', 'is', 'just', 'a', 'tweet', 'with', 'lol']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_quotes_2(self):\n",
    "        self.tokenizer.ignorequotes = False\n",
    "        msg = 'this is just a tweet with \"someone said something funny\" lol'\n",
    "        tks = ['this', 'is', 'just', 'a', 'tweet', 'with', '\"', 'someone',\n",
    "        'said', 'something', 'funny', '\"', 'lol']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_quotes_3(self):\n",
    "        self.tokenizer.ignorequotes = True\n",
    "        msg = ('some stuff but he said â€œyea i know its crazyâ€other '\n",
    "        'stuff...!!! ')\n",
    "        tks = ['some', 'stuff', 'but', 'he', 'said', 'other', 'stuff',\n",
    "        '...', '!', '!', '!']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_quotes_4(self):\n",
    "        self.tokenizer.ignorequotes = True\n",
    "        msg = ('some stuff but he said &ldquo;yea i know its crazy&rdquo;other '\n",
    "        'stuff...!!! ')\n",
    "        tks = ['some', 'stuff', 'but', 'he', 'said', 'other', 'stuff',\n",
    "        '...', '!', '!', '!']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_quotes_5(self):\n",
    "        self.tokenizer.ignorequotes = False\n",
    "        msg = 'heyy buddyyyyy boy \\'do you the lady\\'s kitty like that??\\''\n",
    "        tks = ['heyy', 'buddyyy', 'boy', \"'\", 'do', 'you', 'the',\n",
    "        \"lady's\", 'kitty', 'like', 'that', '?', '?', \"'\"]\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_hashtags_1(self):\n",
    "        msg = 'omg i love#dog#cat#food#other#things#so#fucking#much!!!11LOLOLOL'\n",
    "        tks = ['omg', 'i', 'love', '#dog', '#cat', '#food', '#other',\n",
    "        '#things', '#so', '#fucking', '#much', '!', '!', '!', '11LOLOLOL']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_hashtags_2(self):\n",
    "        self.tokenizer.hashtags = 'HASHTAG'\n",
    "        msg = 'omg i love#dog#cat#food#other#things#so#fucking#much!!!11LOLOLOL'\n",
    "        tks = ['omg', 'i', 'love', 'HASHTAG', 'HASHTAG', 'HASHTAG',\n",
    "        'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', '!', '!', '!',\n",
    "        '11LOLOLOL']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_emoticons_1(self):\n",
    "        msg = 'heyyyyyy:):):(>.<<v.vwhats up man LOL T.T tomcat.tomcat :$;).!!!'\n",
    "        tks = ['heyyy', ':)', ':)', ':(', '>.<', '<', 'v.v', 'whats',\n",
    "        'up', 'man', 'LOL', 'T.T', 'tomcat', '.', 'tomcat', ':$',\n",
    "        ';)', '.', '!', '!', '!']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_removefeatures_1(self):\n",
    "        self.tokenizer.usernames = \"\" # dont' want any usernames to show\n",
    "        msg = ('hey @arnold @nickelodeon #90s#ilove90s#allthat#amandashow'\n",
    "        '@rocko http://en.wikipedia.org/wiki/The_Angry_Beavers ^.^>>><<<^.^')\n",
    "        tks = ['hey', '#90s', '#ilove90s', '#allthat', '#amandashow',\n",
    "        'URL', '^.^', '>', '>', '>', '<', '<', '<', '^.^']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_removefeatures_2(self):\n",
    "        self.tokenizer.usernames = \"\" # dont' want any usernames to show\n",
    "        self.tokenizer.hashtags = \"\"  # or hashtags\n",
    "        msg = ('hey @arnold @nickelodeon #90s#ilove90s#allthat#amandashow'\n",
    "        '@rocko http://en.wikipedia.org/wiki/The_Angry_Beavers ^.^>>><<<^.^')\n",
    "        tks = ['hey', 'URL', '^.^', '>', '>', '>', '<', '<', '<',\n",
    "        '^.^']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_removefeatures_3(self):\n",
    "        self.tokenizer.usernames = False # keep usernames\n",
    "        self.tokenizer.urls = \"\"         # URLs should be removed\n",
    "        self.tokenizer.hashtags = \"$$$\"  # hashtags should be $$$\n",
    "        msg = ('hey @arnold @nickelodeon #90s#ilove90s#allthat#amandashow'\n",
    "        '@rocko http://en.wikipedia.org/wiki/The_Angry_Beavers ^.^>>><<<^.^')\n",
    "        tks = ['hey', '@arnold', '@nickelodeon', '$$$', '$$$', '$$$',\n",
    "        '$$$', '@rocko', '^.^', '>', '>', '>', '<', '<', '<', '^.^']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_emoji_1(self):\n",
    "        msg = ('hey mate!:):3.....@andðŸ‡¨ðŸ‡³ONE+ BRO#loveðŸ˜˜ðŸ˜µðŸ’šðŸ’›ðŸ’œðŸ’™  '\n",
    "        'ðŸ’‹ðŸ˜‚ðŸ˜‚LOLLLL.')\n",
    "        tks = ['hey', 'mate', '!', ':)', ':3', '...',\n",
    "        'USERNAME', '\\U0001f1e8\\U0001f1f3', 'ONE', '+', 'BRO', '#love',\n",
    "        '\\U0001f618', '\\U0001f635', '\\U0001f49a', '\\U0001f49b',\n",
    "        '\\U0001f49c', '\\U0001f499', '\\U0001f48b', '\\U0001f602',\n",
    "        '\\U0001f602', 'LOLLL', '.']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_emoji_2(self):\n",
    "        msg = ('hey mate!:):3.....@andONE+ðŸ‡¬ðŸ‡§  BRO#loveðŸ˜˜ðŸ˜µðŸ’šðŸ’›ðŸ’œðŸ’™  '\n",
    "        'ðŸ’‹ðŸ˜‚ðŸ˜‚LOLLLL.')\n",
    "        tks = ['hey', 'mate', '!', ':)', ':3', '...',\n",
    "        'USERNAME', '+', '\\U0001f1ec\\U0001f1e7', 'BRO', '#love', 'ðŸ˜˜',\n",
    "        'ðŸ˜µ', '\\U0001f49a', '\\U0001f49b', '\\U0001f49c',\n",
    "        '\\U0001f499', 'ðŸ’‹', '\\U0001f602', '\\U0001f602',\n",
    "        'LOLLL', '.']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_emoji_3(self):\n",
    "        msg = ('ðŸš€=)</3O_O:$D:<:-@\\xf0\\x9f\\x98\\xb7ðŸ”¥ðŸ’©ðŸ’… outdated:ðŸ’½ ancient:ðŸ’¾ '\n",
    "        '#getwiththecloud:ðŸ’» and it looks like ðŸ’­')\n",
    "        tks = ['\\U0001f680', '=)', '</3', 'O_O', ':$', 'D:<', ':-@',\n",
    "        '\\U0001f637', '\\U0001f525', '\\U0001f4a9', '\\U0001f485',\n",
    "        'outdated', ':', '\\U0001f4bd', 'ancient', ':',\n",
    "        '\\U0001f4be', '#getwiththecloud',\n",
    "        ':', '\\U0001f4bb', 'and', 'it', 'looks', 'like', '\\U0001f4ad']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_error_1(self):\n",
    "        msg = []\n",
    "        with self.assertRaises(TypeError):\n",
    "            self.tokenizer.tokenize(msg)\n",
    "    \n",
    "    def test_error_2(self):\n",
    "        msg = lambda x: x\n",
    "        with self.assertRaises(TypeError):\n",
    "            self.tokenizer.tokenize(msg)\n",
    "    \n",
    "    def test_actual_tweets_1(self):\n",
    "        \"Number as part of name\"\n",
    "        msg = '@LoganTillman not 2pac and floyd mayweather'\n",
    "        tks = ['USERNAME', 'not', '2pac', 'and', 'floyd', 'mayweather']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_actual_tweets_2(self):\n",
    "        \"Colon no space in hashtag\"\n",
    "        msg = '#MentionSomeoneYoureGladYouMet: @LarryWorld_Wide of course.'\n",
    "        tks = ['#MentionSomeoneYoureGladYouMet', ':', 'USERNAME', 'of',\n",
    "        'course', '.']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "    \n",
    "    def test_stopwords_1(self):\n",
    "        self.tokenizer.ignorestopwords = True\n",
    "        msg = 'i like myself and my so not much and our something he:)'\n",
    "        tks = ['like', 'much', 'something', ':)']\n",
    "        self.assertEqual(self.tokenizer.tokenize(msg), tks)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "msg = ('ðŸš€=)</3O_O:$D:<:-@\\U0001f637ðŸ”¥ðŸ’©ðŸ’… outdated:ðŸ’½ ancient:ðŸ’¾ '\n",
    "'#getwiththecloud:ðŸ’» and it looks like ðŸ’­')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ðŸš€=)</3O_O:$D:<:-@ðŸ˜·ðŸ”¥ðŸ’©ðŸ’… outdated:ðŸ’½ ancient:ðŸ’¾ #getwiththecloud:ðŸ’» and it looks like ðŸ’­'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xf0\\x9f\\x98\\xb7'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ðŸ˜·'.encode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tks = ['\\U0001f680', '=)', '</3', 'O_O', ':$', 'D:<', ':-@',\n",
    "'\\U0001f637', '\\U0001f525', '\\U0001f4a9', '\\U0001f485',\n",
    "'outdated', ':', '\\U0001f4bd', 'ancient', ':',\n",
    "'\\U0001f4be', '#getwiththecloud',\n",
    "':', '\\U0001f4bb', 'and', 'it', 'looks', 'like', '\\U0001f4ad']\n",
    "self.assertEqual(self.tokenizer.tokenize(msg), tks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
