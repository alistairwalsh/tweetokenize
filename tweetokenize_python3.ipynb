{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tweetokenize/tokenizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tweetokenize/tokenizer.py\n",
    "#!/usr/bin/env python\n",
    "#\n",
    "# tweetokenize: Regular expression based tokenizer for Twitter\n",
    "# Copyright: (c) 2013, Jared Suttles. All rights reserved.\n",
    "# License: BSD, see LICENSE for details.\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "import re\n",
    "from os import path\n",
    "\n",
    "from html.entities import name2codepoint\n",
    "\n",
    "html_entities = {k: chr(v) for k, v in name2codepoint.items()}\n",
    "html_entities_re = re.compile(r\"&#?\\w+;\")\n",
    "emoji_ranges = (('\\U0001f300', '\\U0001f5ff'), ('\\U0001f600', '\\U0001f64f'), ('\\U0001f680', '\\U0001f6c5'),\n",
    "                ('\\u2600', '\\u26ff'), ('\\U0001f170', '\\U0001f19a'))\n",
    "emoji_flags =  {'\\U0001f1ef\\U0001f1f5', '\\U0001f1f0\\U0001f1f7', '\\U0001f1e9\\U0001f1ea',\n",
    "                '\\U0001f1e8\\U0001f1f3', '\\U0001f1fa\\U0001f1f8', '\\U0001f1eb\\U0001f1f7',\n",
    "                '\\U0001f1ea\\U0001f1f8', '\\U0001f1ee\\U0001f1f9', '\\U0001f1f7\\U0001f1fa',\n",
    "                '\\U0001f1ec\\U0001f1e7'}\n",
    "\n",
    "def _converthtmlentities(msg):\n",
    "    def replace_entities(s):\n",
    "        s = s.group(0)[1:-1] # remove & and ;\n",
    "        if s[0] == '#':\n",
    "            try:\n",
    "                return chr(int(s[2:],16) if s[1] in 'xX' else int(s[1:]))\n",
    "            except ValueError:\n",
    "                return '&#' + s + ';'\n",
    "        else:\n",
    "            try:\n",
    "                return html_entities[s]\n",
    "            except KeyError:\n",
    "                return '&' + s + ';'\n",
    "    return html_entities_re.sub(replace_entities, msg)\n",
    "\n",
    "def _unicode(word):\n",
    "    if isinstance(word, str):\n",
    "        return word\n",
    "    return str(word, encoding='utf-8')\n",
    "\n",
    "def _isemoji(s):\n",
    "    return len(s) == len('\\U0001f4a9') and any(l <= s <= u for l, u in emoji_ranges) or s in emoji_flags\n",
    "\n",
    "class Tokenizer(object):\n",
    "    \"\"\"\n",
    "    Can be used to tokenize a string representation of a message, adjusting \n",
    "    features based on the given configuration details, to enable further \n",
    "    processing in feature extraction and training stages.\n",
    "    \n",
    "    An example usage::\n",
    "    \n",
    "      >>> from tweetokenize import Tokenizer\n",
    "      >>> gettokens = Tokenizer(usernames='USER', urls='')\n",
    "      >>> gettokens.tokenize('@justinbeiber yo man!love you#inlove#wantyou in a totally straight way #brotime <3:p:D www.justinbeiber.com')\n",
    "      [u'USER', u'yo', u'man', u'!', u'love', u'you', u'#inlove', u'#wantyou', u'in', u'a', u'totally', u'straight', u'way', u'#brotime', u'<3', u':p', u':D']\n",
    "    \"\"\"\n",
    "    _default_args = dict(\n",
    "        lowercase=True, allcapskeep=True, normalize=3, usernames='USERNAME', urls='URL', hashtags=False,\n",
    "        phonenumbers='PHONENUMBER', times='TIME', numbers='NUMBER', ignorequotes=False, ignorestopwords=False\n",
    "    )\n",
    "    _lexicons = path.join(path.dirname(path.realpath(__file__)), 'lexicons/{}.txt')\n",
    "\n",
    "    # Regular expressions\n",
    "    usernames_re = re.compile(r\"@\\w{1,15}\")\n",
    "    with open(_lexicons.format('domains'), 'r') as f:\n",
    "        domains = f.read().strip().replace('\\n', '|')\n",
    "    urls_re = re.compile(r\"(?:(?:https?\\://[A-Za-z0-9\\.]+)|(?:(?:www\\.)?[A-Za-z0-9]+\\.(?:{})))(?:\\/\\S+)?\"\n",
    "                         \"(?=\\s+|$)\".format(domains))\n",
    "    del domains\n",
    "    hashtags_re = re.compile(r\"#\\w+[\\w'-]*\\w+\")\n",
    "    ellipsis_re = re.compile(r\"\\.\\.+\")\n",
    "    word_re = re.compile(r\"(?:[a-zA-Z0-9]+['-]?[a-zA-Z]+[a-zA-Z0-9]*)|(?:[a-zA-Z0-9]*[a-zA-Z]+['-]?[a-zA-Z0-9]+)\")\n",
    "    times_re = re.compile(r\"\\d{1,2}:\\d{2}(?::\\d{2})?\\s*(?:AM|PM|am|pm)?\")\n",
    "    phonenumbers_re = re.compile(r\"(?:\\+?[01][\\-\\s\\.]*)?(?:\\(?\\d{3}[\\-\\s\\.\\)]*)?\\d{3}[\\-\\s\\.]*\\d{4}(?:\\s*x\\s*\\d+)?\"\n",
    "                                 \"(?=\\s+|$)\")\n",
    "    number_re = r\"(?:[+-]?\\$?\\d+(?:\\.\\d+)?(?:[eE]-?\\d+)?%?)(?![A-Za-z])\"\n",
    "    numbers_re = re.compile(r\"{0}(?:\\s*/\\s*{0})?\".format(number_re))  # deals with fractions\n",
    "    del number_re\n",
    "    other_re = r\"(?:[^#\\s\\.]|\\.(?!\\.))+\"\n",
    "    _token_regexs = ('usernames', 'urls', 'hashtags', 'times', 'phonenumbers', 'numbers')\n",
    "    tokenize_re = re.compile(\n",
    "        r\"|\".join(\n",
    "            map(lambda x: getattr(x, 'pattern', x),\n",
    "                 [l[regex + '_re'] for l in (locals(),) for regex in ('usernames', 'urls', 'hashtags', 'times', 'phonenumbers', 'numbers')] + [word_re, ellipsis_re, other_re])))\n",
    "    #del regex  # otherwise stays in class namespace\n",
    "    repeating_re = re.compile(r\"([a-zA-Z])\\1\\1+\")\n",
    "    doublequotes = (('‚Äú','‚Äù'),('\"','\"'),('‚Äò','‚Äô'),('ÔºÇ','ÔºÇ'))\n",
    "    punctuation = ('!$%()*+,-/:;<=>?[\\\\]^_.`{|}~\\'' + ''.join(c for t in doublequotes for c in t))\n",
    "    quotes_re = re.compile(r\"|\".join(r'({}.*?{})'.format(f,s) for f,s in doublequotes) + r'|\\s(\\'.*?\\')\\s')\n",
    "    del doublequotes\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Constructs a new Tokenizer. Can specify custom settings for various \n",
    "        feature normalizations.\n",
    "        \n",
    "        Any features with replacement tokens can be removed from the message by \n",
    "        setting the token to the empty string (C{\"\"}), C{\"DELETE\"}, or \n",
    "        C{\"REMOVE\"}.\n",
    "        \n",
    "        @type lowercase: C{bool}\n",
    "        @param lowercase: If C{True}, lowercases words, excluding those with \n",
    "            all letters capitalized.\n",
    "        \n",
    "        @type allcapskeep: C{bool}\n",
    "        @param allcapskeep: If C{True}, maintains capitalization for words with \n",
    "            all letters in capitals. Otherwise, capitalization for such words \n",
    "            is dependent on C{lowercase}.\n",
    "        \n",
    "        @type normalize: C{int}\n",
    "        @param normalize: The number of repeating letters when normalizing \n",
    "            arbitrary letter elongations.\n",
    "            \n",
    "            Example::\n",
    "                Heyyyyyy i lovvvvvvve youuuuuuuuu <3\n",
    "            \n",
    "            Becomes::\n",
    "                Heyyy i lovvve youuu <3\n",
    "            \n",
    "            Not sure why you would want to change this (maybe just for fun?? :P)\n",
    "        \n",
    "        @param usernames: Serves as the replacement token for anything that \n",
    "            parses as a Twitter username, ie. C{@rayj}. Setting this to \n",
    "            C{False} means no usernames will be changed.\n",
    "        \n",
    "        @param urls: Serves as the replacement token for anything that \n",
    "            parses as a URL, ie. C{bit.ly} or C{http://example.com}. Setting \n",
    "            this to C{False} means no URLs will be changed.\n",
    "        \n",
    "        @param hashtags: Serves as the replacement token for anything that \n",
    "            parses as a Twitter hashtag, ie. C{#ihititfirst} or \n",
    "            C{#onedirection}. Setting this to C{False} means no hashtags will \n",
    "            be changed.\n",
    "        \n",
    "        @param phonenumbers: Replacement token for phone numbers.\n",
    "        \n",
    "        @param times: Replacement token for times.\n",
    "        \n",
    "        @param numbers: Replacement token for any other kinds of numbers.\n",
    "        \n",
    "        @type ignorequotes: C{bool}\n",
    "        @param ignorequotes: If C{True}, will remove various types of quotes \n",
    "            and the contents within.\n",
    "        \n",
    "        @type ignorestopwords: C{bool}\n",
    "        @param ignorestopwords: If C{True}, will remove any stopwords. The \n",
    "            default set includes 'I', 'me', 'itself', 'against', 'should', etc.\n",
    "        \"\"\"\n",
    "        for keyword in self._default_args:\n",
    "            setattr(self, keyword, kwargs.get(keyword, self._default_args[keyword]))\n",
    "        self.emoticons(filename=self._lexicons.format('emoticons'))\n",
    "        self.stopwords(filename=self._lexicons.format('stopwords'))\n",
    "\n",
    "    def __call__(self, iterable):\n",
    "        \"\"\"\n",
    "        Iterator for the tokenization of given messages.\n",
    "        \n",
    "        @rtype: C{list} of C{str}\n",
    "        @return: Iterator of lists representing message tokenizations.\n",
    "        \n",
    "        @param iterable: Object capable of iteration, providing strings for \n",
    "            tokenization.\n",
    "        \"\"\"\n",
    "        for msg in iterable:\n",
    "            yield self.tokenize(msg)\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Adjust any settings of the Tokenizer.\n",
    "\n",
    "          >>> gettokens = Tokenizer())\n",
    "          >>> gettokens.lowercase\n",
    "          True\n",
    "          >>> gettokens.phonenumbers\n",
    "          'PHONENUMBER'\n",
    "          >>> gettokens.update(phonenumbers='NUMBER', lowercase=False)\n",
    "          >>> gettokens.lowercase\n",
    "          False\n",
    "          >>> gettokens.phonenumbers\n",
    "          'NUMBER'\n",
    "        \"\"\"\n",
    "        for keyword in self._default_args:\n",
    "            if keyword in kwargs:\n",
    "                setattr(self, keyword, kwargs[keyword])\n",
    "\n",
    "    def _replacetokens(self, msg):\n",
    "        tokens = []\n",
    "        deletion_tokens = {'', 'REMOVE', 'remove', 'DELETE', 'delete'}\n",
    "        for word in msg:\n",
    "            matching = self.word_re.match(word) # 1st check if normal word\n",
    "            if matching and len(matching.group(0)) == len(word):\n",
    "                tokens.append(self._cleanword(word))\n",
    "                continue # don't check rest of conditions\n",
    "            for token in self._token_regexs: # id & possibly replace tokens\n",
    "                regex = getattr(self, token + '_re')\n",
    "                replacement_token = getattr(self, token)\n",
    "                if regex.match(word):\n",
    "                    if replacement_token: # decide if we change it\n",
    "                        word = _unicode(str(replacement_token))\n",
    "                    if replacement_token not in deletion_tokens:\n",
    "                        tokens.append(word)\n",
    "                    break\n",
    "            else: # we didn't find a match for any token so far...\n",
    "                if self.ellipsis_re.match(word):\n",
    "                    tokens.append(\"...\")\n",
    "                else: # split into tokens based on emoticons or punctuation\n",
    "                    tokens.extend(self._separate_emoticons_punctuation(word))\n",
    "        return tokens\n",
    "\n",
    "    def _separate_emoticons_punctuation(self, word):\n",
    "        newwords, wordbefore = [], []\n",
    "        i = 0\n",
    "        def possibly_append_and_reset():\n",
    "            if wordbefore:\n",
    "                newwords.append(self._cleanword(''.join(wordbefore)))\n",
    "                wordbefore[:] = []\n",
    "        while i < len(word):\n",
    "            # greedily check for emoticons in this word\n",
    "            for l in range(self._maxlenemo, 0, -1):\n",
    "                if word[i:i+l] in self._emoticons or _isemoji(word[i:i+l]):\n",
    "                    possibly_append_and_reset()\n",
    "                    newwords.append(word[i:i+l])\n",
    "                    i+=l\n",
    "                    break\n",
    "            else: # its safe to break up any punctuation not part of emoticons\n",
    "                if word[i] in self.punctuation:\n",
    "                    possibly_append_and_reset()\n",
    "                    newwords.append(word[i])\n",
    "                else:\n",
    "                    wordbefore.append(word[i])\n",
    "                i+=1\n",
    "        # possible ending of word which wasn't emoticon or punctuation\n",
    "        possibly_append_and_reset()\n",
    "        return newwords\n",
    "\n",
    "    def _cleanword(self, word):\n",
    "        if self.normalize: # replace characters with >=3 alphabetic repeating\n",
    "            word = self.repeating_re.sub(r\"\\1\"*self.normalize, word)\n",
    "        if self.lowercase and (not self.allcapskeep or not word.isupper()):\n",
    "            return word.lower()\n",
    "        return word\n",
    "\n",
    "    def tokenize(self, message):\n",
    "        \"\"\"\n",
    "        Tokenize the given string into a list of strings representing the \n",
    "        constituent words of the message.\n",
    "        \n",
    "        @rtype: C{list} of C{str}\n",
    "        @return: The tokenization of the message.\n",
    "        \n",
    "        @type message: C{str}\n",
    "        @param message: The string representation of the message.\n",
    "        \"\"\"\n",
    "        if not isinstance(message, str):\n",
    "            raise TypeError('cannot tokenize non-string, {}'.format(repr(type(message).__name__)))\n",
    "        message = _converthtmlentities(_unicode(message))\n",
    "        if self.ignorequotes:\n",
    "            message = self.quotes_re.sub(\" \", message)\n",
    "        message = self._replacetokens(self.tokenize_re.findall(message))\n",
    "        if self.ignorestopwords:\n",
    "            message = [word for word in message if word not in self._stopwords]\n",
    "        return message\n",
    "\n",
    "    def emoticons(self, iterable=None, filename=None):\n",
    "        \"\"\"\n",
    "        Consumes an iterable of emoticons that the tokenizer will tokenize on. \n",
    "        Allows for user-specified set of emoticons to be recognized.\n",
    "        \n",
    "        @param iterable: Object capable of iteration, providing emoticon \n",
    "            strings.\n",
    "        @type filename: C{str}\n",
    "        @param filename: Path to the file containing emoticons delimited by \n",
    "            new lines. Strips trailing whitespace and skips blank lines.\n",
    "        \"\"\"\n",
    "        self._emoticons = self._collectset(iterable, filename)\n",
    "        self._maxlenemo = max(len(max(self._emoticons, key=lambda x: len(x))),\n",
    "        len('\\U0001f1e8\\U0001f1f3'), len('\\U0001f48b'))\n",
    "\n",
    "    def stopwords(self, iterable=None, filename=None):\n",
    "        \"\"\"\n",
    "        Consumes an iterable of stopwords that the tokenizer will ignore if the \n",
    "        stopwords setting is C{True}. The default set is taken from NLTK's \n",
    "        english list.\n",
    "        \n",
    "        @param iterable: Object capable of iteration, providing stopword \n",
    "            strings.\n",
    "        @type filename: C{str}\n",
    "        @param filename: Path to the file containing stopwords delimited by \n",
    "            new lines. Strips trailing whitespace and skips blank lines.\n",
    "        \"\"\"\n",
    "        self._stopwords = self._collectset(iterable, filename)\n",
    "\n",
    "    @staticmethod\n",
    "    def _collectset(iterable, filename):\n",
    "        if filename:\n",
    "            with open(filename, \"r\") as f:\n",
    "                iterable = set(l.rstrip() for l in f)\n",
    "                iterable.discard('')\n",
    "        return set(map(_unicode, iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lexicons = '/Users/alistair/Documents/git_repos/tweetokenize/tweetokenize/lexicons/{}.txt'\n",
    "# Regular expressions\n",
    "usernames_re = re.compile(r\"@\\w{1,15}\")\n",
    "with open(_lexicons.format('domains'), 'r') as f:\n",
    "    domains = f.read().strip().replace('\\n', '|')\n",
    "urls_re = re.compile(r\"(?:(?:https?\\://[A-Za-z0-9\\.]+)|(?:(?:www\\.)?[A-Za-z0-9]+\\.(?:{})))(?:\\/\\S+)?\"\n",
    "                     \"(?=\\s+|$)\".format(domains))\n",
    "del domains\n",
    "hashtags_re = re.compile(r\"#\\w+[\\w'-]*\\w+\")\n",
    "ellipsis_re = re.compile(r\"\\.\\.+\")\n",
    "word_re = re.compile(r\"(?:[a-zA-Z0-9]+['-]?[a-zA-Z]+[a-zA-Z0-9]*)|(?:[a-zA-Z0-9]*[a-zA-Z]+['-]?[a-zA-Z0-9]+)\")\n",
    "times_re = re.compile(r\"\\d{1,2}:\\d{2}(?::\\d{2})?\\s*(?:AM|PM|am|pm)?\")\n",
    "phonenumbers_re = re.compile(r\"(?:\\+?[01][\\-\\s\\.]*)?(?:\\(?\\d{3}[\\-\\s\\.\\)]*)?\\d{3}[\\-\\s\\.]*\\d{4}(?:\\s*x\\s*\\d+)?\"\n",
    "                             \"(?=\\s+|$)\")\n",
    "number_re = r\"(?:[+-]?\\$?\\d+(?:\\.\\d+)?(?:[eE]-?\\d+)?%?)(?![A-Za-z])\"\n",
    "numbers_re = re.compile(r\"{0}(?:\\s*/\\s*{0})?\".format(number_re))  # deals with fractions\n",
    "del number_re\n",
    "other_re = r\"(?:[^#\\s\\.]|\\.(?!\\.))+\"\n",
    "_token_regexs = ('usernames', 'urls', 'hashtags', 'times', 'phonenumbers', 'numbers')\n",
    "\n",
    "tokenize_re = re.compile(\n",
    "    r\"|\".join(\n",
    "        map(lambda x: getattr(x, 'pattern', x),\n",
    "             [locals_dict[regex + '_re'] for regex in _token_regexs] + [word_re, ellipsis_re, other_re])))\n",
    "#del regex  # otherwise stays in class namespace\n",
    "repeating_re = re.compile(r\"([a-zA-Z])\\1\\1+\")\n",
    "doublequotes = (('‚Äú','‚Äù'),('\"','\"'),('‚Äò','‚Äô'),('ÔºÇ','ÔºÇ'))\n",
    "punctuation = ('!$%()*+,-/:;<=>?[\\\\]^_.`{|}~\\'' + ''.join(c for t in doublequotes for c in t))\n",
    "quotes_re = re.compile(r\"|\".join(r'({}.*?{})'.format(f,s) for f,s in doublequotes) + r'|\\s(\\'.*?\\')\\s')\n",
    "del doublequotes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "listoregx = []\n",
    "for regx in _token_regexs:\n",
    "    listoregx.append(locals()[regx + '_re'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[re.compile(r'@\\w{1,15}', re.UNICODE),\n",
       " re.compile(r'(?:(?:https?\\://[A-Za-z0-9\\.]+)|(?:(?:www\\.)?[A-Za-z0-9]+\\.(?:museum|travel|aero|arpa|asia|coop|info|jobs|mobi|name|post|biz|cat|com|edu|gov|int|mil|net|org|pro|tel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cu|cv|cw|cx|cy|cz|de|dj|dk|dm|do|dz|ec|ee|eg|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|sk|sl|sm|sn|so|sr|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|za|zm|zw)))(?:\\/\\S+)?(?=\\s+|$)',\n",
       " re.UNICODE),\n",
       " re.compile(r\"#\\w+[\\w'-]*\\w+\", re.UNICODE),\n",
       " re.compile(r'\\d{1,2}:\\d{2}(?::\\d{2})?\\s*(?:AM|PM|am|pm)?', re.UNICODE),\n",
       " re.compile(r'(?:\\+?[01][\\-\\s\\.]*)?(?:\\(?\\d{3}[\\-\\s\\.\\)]*)?\\d{3}[\\-\\s\\.]*\\d{4}(?:\\s*x\\s*\\d+)?(?=\\s+|$)',\n",
       " re.UNICODE),\n",
       " re.compile(r'(?:[+-]?\\$?\\d+(?:\\.\\d+)?(?:[eE]-?\\d+)?%?)(?![A-Za-z])(?:\\s*/\\s*(?:[+-]?\\$?\\d+(?:\\.\\d+)?(?:[eE]-?\\d+)?%?)(?![A-Za-z]))?',\n",
       " re.UNICODE)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listoregx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '_ih', '_oh', '_dh', '_sh', 'In', 'Out', 'get_ipython', 'exit', 'quit', '_', '__', '___', '_i', '_ii', '_iii', '_i1', '_i2', '_i3', 're', 'path', 'name2codepoint', 'html_entities', 'html_entities_re', 'emoji_ranges', 'emoji_flags', '_converthtmlentities', '_unicode', '_isemoji', '_i4', '_i5', '_5', '_i6', '_i7', '_7', '_i8', '_i9', '_i10', '_i11', '_i12', '_12', '_i13', '_i14', '_i15', '__file__', '_i16', '_i17', '_i18', '_i19', 'os', '_i20', '_i21', '_i22', '_i23', '_23', '_i24', '_i25', '_i26', '_i27', '_token_regexs', '_i28', '_28', '_i29', '_29', '_i30', '_30', '_i31', '_i32', 'usernames_re', '_i33', '_lexicons', 'f', 'urls_re', 'hashtags_re', 'ellipsis_re', 'word_re', 'times_re', 'phonenumbers_re', 'numbers_re', 'other_re', '_i34', '_34', '_i35'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locals().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'@\\w{1,15}', re.UNICODE)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locals()['usernames_re']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@|\\\\|w|{|1|,|1|5|}'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r\"|\".join(getattr(listoregx[0], 'pattern', listoregx[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@\\\\w{1,15}|(?:(?:https?\\\\://[A-Za-z0-9\\\\.]+)|(?:(?:www\\\\.)?[A-Za-z0-9]+\\\\.(?:museum|travel|aero|arpa|asia|coop|info|jobs|mobi|name|post|biz|cat|com|edu|gov|int|mil|net|org|pro|tel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cu|cv|cw|cx|cy|cz|de|dj|dk|dm|do|dz|ec|ee|eg|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|sk|sl|sm|sn|so|sr|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|za|zm|zw)))(?:\\\\/\\\\S+)?(?=\\\\s+|$)|#\\\\w+[\\\\w'-]*\\\\w+|\\\\d{1,2}:\\\\d{2}(?::\\\\d{2})?\\\\s*(?:AM|PM|am|pm)?|(?:\\\\+?[01][\\\\-\\\\s\\\\.]*)?(?:\\\\(?\\\\d{3}[\\\\-\\\\s\\\\.\\\\)]*)?\\\\d{3}[\\\\-\\\\s\\\\.]*\\\\d{4}(?:\\\\s*x\\\\s*\\\\d+)?(?=\\\\s+|$)|(?:[+-]?\\\\$?\\\\d+(?:\\\\.\\\\d+)?(?:[eE]-?\\\\d+)?%?)(?![A-Za-z])(?:\\\\s*/\\\\s*(?:[+-]?\\\\$?\\\\d+(?:\\\\.\\\\d+)?(?:[eE]-?\\\\d+)?%?)(?![A-Za-z]))?\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r\"|\".join([getattr(x, 'pattern', x) for x in listoregx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[re.compile(r'@\\w{1,15}', re.UNICODE),\n",
       " re.compile(r'(?:(?:https?\\://[A-Za-z0-9\\.]+)|(?:(?:www\\.)?[A-Za-z0-9]+\\.(?:museum|travel|aero|arpa|asia|coop|info|jobs|mobi|name|post|biz|cat|com|edu|gov|int|mil|net|org|pro|tel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cu|cv|cw|cx|cy|cz|de|dj|dk|dm|do|dz|ec|ee|eg|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|sk|sl|sm|sn|so|sr|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|za|zm|zw)))(?:\\/\\S+)?(?=\\s+|$)',\n",
       " re.UNICODE),\n",
       " re.compile(r\"#\\w+[\\w'-]*\\w+\", re.UNICODE),\n",
       " re.compile(r'\\d{1,2}:\\d{2}(?::\\d{2})?\\s*(?:AM|PM|am|pm)?', re.UNICODE),\n",
       " re.compile(r'(?:\\+?[01][\\-\\s\\.]*)?(?:\\(?\\d{3}[\\-\\s\\.\\)]*)?\\d{3}[\\-\\s\\.]*\\d{4}(?:\\s*x\\s*\\d+)?(?=\\s+|$)',\n",
       " re.UNICODE),\n",
       " re.compile(r'(?:[+-]?\\$?\\d+(?:\\.\\d+)?(?:[eE]-?\\d+)?%?)(?![A-Za-z])(?:\\s*/\\s*(?:[+-]?\\$?\\d+(?:\\.\\d+)?(?:[eE]-?\\d+)?%?)(?![A-Za-z]))?',\n",
       " re.UNICODE)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locals_dict = locals()\n",
    "[locals_dict[regex + '_re'] for regex in _token_regexs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locals_dict = locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/Users/alistair/anaconda/envs/twitter/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 672\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alistair/anaconda/envs/twitter/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    366\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                     \u001b[0;31m# printer registered in self.type_pprinters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0;31m# deferred printer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alistair/anaconda/envs/twitter/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alistair/anaconda/envs/twitter/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    366\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                     \u001b[0;31m# printer registered in self.type_pprinters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0;31m# deferred printer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alistair/anaconda/envs/twitter/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alistair/anaconda/envs/twitter/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    381\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                                 \u001b[0;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alistair/anaconda/envs/twitter/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_default_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_safe_getattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__repr__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_baseclass_reprs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;31m# A user-provided repr. Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0m_repr_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alistair/anaconda/envs/twitter/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alistair/anaconda/envs/twitter/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_module_repr\u001b[0;34m(module)\u001b[0m\n",
      "\u001b[0;32m/Users/alistair/anaconda/envs/twitter/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_module_repr_from_spec\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
     ]
    }
   ],
   "source": [
    "locals_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@\\\\w{1,15}',\n",
       " '(?:(?:https?\\\\://[A-Za-z0-9\\\\.]+)|(?:(?:www\\\\.)?[A-Za-z0-9]+\\\\.(?:museum|travel|aero|arpa|asia|coop|info|jobs|mobi|name|post|biz|cat|com|edu|gov|int|mil|net|org|pro|tel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cu|cv|cw|cx|cy|cz|de|dj|dk|dm|do|dz|ec|ee|eg|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|sk|sl|sm|sn|so|sr|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|za|zm|zw)))(?:\\\\/\\\\S+)?(?=\\\\s+|$)',\n",
       " \"#\\\\w+[\\\\w'-]*\\\\w+\",\n",
       " '\\\\d{1,2}:\\\\d{2}(?::\\\\d{2})?\\\\s*(?:AM|PM|am|pm)?',\n",
       " '(?:\\\\+?[01][\\\\-\\\\s\\\\.]*)?(?:\\\\(?\\\\d{3}[\\\\-\\\\s\\\\.\\\\)]*)?\\\\d{3}[\\\\-\\\\s\\\\.]*\\\\d{4}(?:\\\\s*x\\\\s*\\\\d+)?(?=\\\\s+|$)',\n",
       " '(?:[+-]?\\\\$?\\\\d+(?:\\\\.\\\\d+)?(?:[eE]-?\\\\d+)?%?)(?![A-Za-z])(?:\\\\s*/\\\\s*(?:[+-]?\\\\$?\\\\d+(?:\\\\.\\\\d+)?(?:[eE]-?\\\\d+)?%?)(?![A-Za-z]))?',\n",
       " \"(?:[a-zA-Z0-9]+['-]?[a-zA-Z]+[a-zA-Z0-9]*)|(?:[a-zA-Z0-9]*[a-zA-Z]+['-]?[a-zA-Z0-9]+)\",\n",
       " '\\\\.\\\\.+',\n",
       " '(?:[^#\\\\s\\\\.]|\\\\.(?!\\\\.))+']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[getattr(x, 'pattern', x) for x in [l[regex + '_re']\\\n",
    "                                    for l in (locals(),) \\\n",
    "                                    for regex in _token_regexs] + [word_re, ellipsis_re, other_re]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[locals_dict[regex + '_re'] for regex in _token_regexs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[re.compile(r'@\\w{1,15}', re.UNICODE),\n",
       " re.compile(r'(?:(?:https?\\://[A-Za-z0-9\\.]+)|(?:(?:www\\.)?[A-Za-z0-9]+\\.(?:museum|travel|aero|arpa|asia|coop|info|jobs|mobi|name|post|biz|cat|com|edu|gov|int|mil|net|org|pro|tel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cu|cv|cw|cx|cy|cz|de|dj|dk|dm|do|dz|ec|ee|eg|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|sk|sl|sm|sn|so|sr|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|za|zm|zw)))(?:\\/\\S+)?(?=\\s+|$)',\n",
       " re.UNICODE),\n",
       " re.compile(r\"#\\w+[\\w'-]*\\w+\", re.UNICODE),\n",
       " re.compile(r'\\d{1,2}:\\d{2}(?::\\d{2})?\\s*(?:AM|PM|am|pm)?', re.UNICODE),\n",
       " re.compile(r'(?:\\+?[01][\\-\\s\\.]*)?(?:\\(?\\d{3}[\\-\\s\\.\\)]*)?\\d{3}[\\-\\s\\.]*\\d{4}(?:\\s*x\\s*\\d+)?(?=\\s+|$)',\n",
       " re.UNICODE),\n",
       " re.compile(r'(?:[+-]?\\$?\\d+(?:\\.\\d+)?(?:[eE]-?\\d+)?%?)(?![A-Za-z])(?:\\s*/\\s*(?:[+-]?\\$?\\d+(?:\\.\\d+)?(?:[eE]-?\\d+)?%?)(?![A-Za-z]))?',\n",
       " re.UNICODE)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[l[regex + '_re'] for l in (locals(),) for regex in _token_regexs] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "_token_regexs = ('usernames', 'urls', 'hashtags', 'times', 'phonenumbers', 'numbers')\n",
    "tokenize_re = re.compile(\n",
    "    r\"|\".join(\n",
    "        map(lambda x: getattr(x, 'pattern', x),\n",
    "             [l[regex + '_re'] for l in (locals(),) for regex in _token_regexs] + [word_re, ellipsis_re, other_re])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r\"@\\w{1,15}|(?:(?:https?\\://[A-Za-z0-9\\.]+)|(?:(?:www\\.)?[A-Za-z0-9]+\\.(?:museum|travel|aero|arpa|asia|coop|info|jobs|mobi|name|post|biz|cat|com|edu|gov|int|mil|net|org|pro|tel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cu|cv|cw|cx|cy|cz|de|dj|dk|dm|do|dz|ec|ee|eg|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|sk|sl|sm|sn|so|sr|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|za|zm|zw)))(?:\\/\\S+)?(?=\\s+|$)|#\\w+[\\w'-]*\\w+|\\d{1,2}:\\d{2}(?::\\d{2})?\\s*(?:AM|PM|am|pm)?|(?:\\+?[01][\\-\\s\\.]*)?(?:\\(?\\d{3}[\\-\\s\\.\\)]*)?\\d{3}[\\-\\s\\.]*\\d{4}(?:\\s*x\\s*\\d+)?(?=\\s+|$)|(?:[+-]?\\$?\\d+(?:\\.\\d+)?(?:[eE]-?\\d+)?%?)(?![A-Za-z])(?:\\s*/\\s*(?:[+-]?\\$?\\d+(?:\\.\\d+)?(?:[eE]-?\\d+)?%?)(?![A-Za-z]))?|(?:[a-zA-Z0-9]+['-]?[a-zA-Z]+[a-zA-Z0-9]*)|(?:[a-zA-Z0-9]*[a-zA-Z]+['-]?[a-zA-Z0-9]+)|\\.\\.+|(?:[^#\\s\\.]|\\.(?!\\.))+\",\n",
       "re.UNICODE)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√∞¬ü¬ò¬∑\n"
     ]
    }
   ],
   "source": [
    "print(u'√∞\\x9f\\x98¬∑')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'√∞\\x9f\\x98¬∑'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xf0\\x9f\\x98\\xb7'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'üò∑'.encode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</3O_O:$D:<:-@üò∑'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u'</3O_O:$D:<:-@üò∑'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode character '\\U0001f637' in position 0: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-10cab7d3a4d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m'üò∑'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character '\\U0001f637' in position 0: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "'üò∑'.encode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
